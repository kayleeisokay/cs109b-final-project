{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4111f1c",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73213c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "def visualize_outliers(num_examples, traingen, batch_size, outlier_indices):\n",
    "    fig, axs = plt.subplots(2, num_examples, figsize=(15, 6))\n",
    "\n",
    "    # Find indices that are no outliers\n",
    "    inlier_indices = np.setdiff1d(\n",
    "        np.arange(len(traingen) * batch_size), outlier_indices\n",
    "    )\n",
    "    inlier_indices = inlier_indices[:num_examples]\n",
    "\n",
    "    counter = 0\n",
    "    for idx in outlier_indices[:num_examples]:\n",
    "        # Get the batch index and image index in the batch\n",
    "        batch_idx = idx // batch_size\n",
    "        image_idx_in_batch = idx % batch_size\n",
    "\n",
    "        # Fetch the image from the generator's batch\n",
    "        batch = traingen[batch_idx]\n",
    "        original_image = batch[0][image_idx_in_batch]\n",
    "\n",
    "        # Plot original image\n",
    "        axs[0, counter].imshow(original_image[:, :, 0], cmap=\"gray\")\n",
    "        axs[0, counter].set_title(\"Outlier\")\n",
    "        axs[0, counter].axis(\"off\")\n",
    "\n",
    "        # Plot non-outlier (inlier) image\n",
    "        inlier_idx = inlier_indices[counter]\n",
    "        inlier_batch_idx = inlier_idx // batch_size\n",
    "        inlier_image_idx = inlier_idx % batch_size\n",
    "        inlier_image = traingen[inlier_batch_idx][0][inlier_image_idx]\n",
    "\n",
    "        axs[1, counter].imshow(inlier_image[:, :, 0], cmap=\"gray\")\n",
    "        axs[1, counter].set_title(\"Non-Outlier\")\n",
    "        axs[1, counter].axis(\"off\")\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    fig.suptitle(\"Comparison: Outlier Images vs. Kept Images\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def output_clean_train(DATA_DIR, traingen, outlier_indices):\n",
    "    src_dir = Path(DATA_DIR) / \"train\"\n",
    "    dst_dir = Path(DATA_DIR) / \"clean_train\"\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    # Get filenames\n",
    "    filenames = traingen.filenames\n",
    "    outlier_set = set(outlier_indices)\n",
    "\n",
    "    # Copy non-outlier images\n",
    "    for idx, rel_path in tqdm(enumerate(filenames), total=len(filenames)):\n",
    "        # If outlier, skip\n",
    "        if idx in outlier_set:\n",
    "            continue\n",
    "\n",
    "        src_path = src_dir / rel_path\n",
    "        dst_class_dir = dst_dir / rel_path.split(\"/\")[0]\n",
    "        dst_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        dst_path = dst_class_dir / Path(rel_path).name\n",
    "        # move file\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"Copied {len(filenames) - len(outlier_set)} non-outlier images to {dst_dir}\")\n",
    "\n",
    "\n",
    "# plot accuracy\n",
    "def plot_accuracy(history, plot_title=\"Model\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot accuracy of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    best_val_acc = np.max(history.history[\"val_accuracy\"])\n",
    "    best_epoch = np.argmax(history.history[\"val_accuracy\"])\n",
    "\n",
    "    epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
    "\n",
    "    plt.plot(epochs, history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(epochs, history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "\n",
    "    plt.axvline(\n",
    "        best_epoch + 1,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best val acc: {best_val_acc:.4f}\",\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{plot_title}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(epochs)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "def plot_accuracy_and_loss(history, plot_title=\"Model Training History\"):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Extract metrics from history\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    best_epoch_acc = np.argmax(val_acc) + 1\n",
    "    best_val_acc = val_acc[best_epoch_acc - 1]\n",
    "\n",
    "    plt.plot(epochs, acc, label=\"Training Accuracy\", linewidth=2)\n",
    "    plt.plot(epochs, val_acc, label=\"Validation Accuracy\", linewidth=2)\n",
    "    plt.axvline(\n",
    "        x=best_epoch_acc,\n",
    "        color=\"k\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best Val Acc: {best_val_acc:.4f} at Epoch {best_epoch_acc}\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Training and Validation Accuracy\", fontsize=14)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    best_epoch_loss = np.argmin(val_loss) + 1\n",
    "    best_val_loss = val_loss[best_epoch_loss - 1]\n",
    "\n",
    "    plt.plot(epochs, loss, label=\"Training Loss\", linewidth=2)\n",
    "    plt.plot(epochs, val_loss, label=\"Validation Loss\", linewidth=2)\n",
    "    plt.axvline(\n",
    "        x=best_epoch_loss,\n",
    "        color=\"k\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Best Val Loss: {best_val_loss:.4f} at Epoch {best_epoch_loss}\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Training and Validation Loss\", fontsize=14)\n",
    "    plt.xlabel(\"Epoch\", fontsize=12)\n",
    "    plt.ylabel(\"Loss\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "    plt.suptitle(plot_title, fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71002d8",
   "metadata": {},
   "source": [
    "## Vision Transformer Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47195407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SingleHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        \"\"\"\n",
    "        Single-head attention layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimensionality for queries, keys, and values.\n",
    "        \"\"\"\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        # hidden_dim: The dimension of the projected tensors.\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # W_value: A dense layer mapping inputs to tensors with dimension hidden_dim\n",
    "        self.W_query = tf.keras.layers.Dense(hidden_dim)\n",
    "        # W_key: A dense layer mapping inputs to tensors with dimension hidden_dim\n",
    "        self.W_key = tf.keras.layers.Dense(hidden_dim)\n",
    "        # W_value: A dense layer mapping inputs to tensors with dimension hidden_dim\n",
    "        self.W_value = tf.keras.layers.Dense(hidden_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for single-head attention.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            attention_output: Attention output of shape (batch_size, seq_length, hidden_dim).\n",
    "        \"\"\"\n",
    "        # Linear Projections\n",
    "        Q = self.W_query(inputs)\n",
    "        K = self.W_key(inputs)\n",
    "        V = self.W_value(inputs)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attention_scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(\n",
    "            tf.cast(self.hidden_dim, tf.float32)\n",
    "        )\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        attention_output = tf.matmul(attention_weights, V)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, head_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-head attention layer.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimensionality of the input and final output.\n",
    "            head_dim: Dimensionality for each attention head.\n",
    "            num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        # Each head is an instance of your single-head attention layer,\n",
    "        # configured with a projection output dimensionality of head_dim.\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Create a list of attention heads. Each head is an instance of your single-head attention layer,\n",
    "        # configured with a projection output dimensionality of head_dim.\n",
    "        self.heads = [SingleHeadAttention(head_dim) for _ in range(num_heads)]\n",
    "\n",
    "        # Define a Dense layer that projects the concatenated outputs\n",
    "        # back to the original input dimension (input_dim).\n",
    "        self.output_projection = tf.keras.layers.Dense(input_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            multihead_output: Multi-head attention output of shape (batch_size, seq_length, input_dim).\n",
    "        \"\"\"\n",
    "        head_outputs = [head(inputs) for head in self.heads]\n",
    "        concat_output = tf.concat(head_outputs, axis=-1)\n",
    "        multihead_output = self.output_projection(concat_output)\n",
    "\n",
    "        return multihead_output\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        attention_dim,\n",
    "        feedforward_dim,\n",
    "        num_heads,\n",
    "        ffn_dropout_rate=0.1,\n",
    "        attn_dropout_rate=0.1,\n",
    "        epsilon=1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A single Transformer encoder block with multi-head attention and feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        # for call feature later\n",
    "        self.attn_norm = tf.keras.layers.LayerNormalization(epsilon=epsilon)\n",
    "\n",
    "        # Create a MultiHeadAttentionLayer that processes the inputs\n",
    "        # to produce attn_output = MultiHeadAttentionLayer(inputs)\n",
    "        self.attention = MultiHeadAttentionLayer(\n",
    "            input_dim=embed_dim, head_dim=attention_dim, num_heads=num_heads\n",
    "        )\n",
    "        # for call feature later\n",
    "        self.attn_dropout = tf.keras.layers.Dropout(attn_dropout_rate)\n",
    "\n",
    "        # Build a Sequential block consisting of:\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.LayerNormalization(\n",
    "                    epsilon=epsilon\n",
    "                ),  # LayerNormalization with epsilon = 1e-5\n",
    "                tf.keras.layers.Dense(\n",
    "                    feedforward_dim, activation=tf.nn.gelu\n",
    "                ),  # A Dense layer with GELU activation mapping to feedforward_dim units\n",
    "                tf.keras.layers.Dropout(\n",
    "                    ffn_dropout_rate\n",
    "                ),  # A Dropout layer with ffn_dropout_rate.\n",
    "                tf.keras.layers.Dense(\n",
    "                    embed_dim\n",
    "                ),  # A Dense layer mapping back to embed_dim\n",
    "                tf.keras.layers.Dropout(\n",
    "                    ffn_dropout_rate\n",
    "                ),  # A Dropout layer with ffn_dropout_rate.\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass for a single Transformer encoder block.\n",
    "        \"\"\"\n",
    "        # Compute the Multi-Head Attention\n",
    "        normalized_inputs = self.attn_norm(\n",
    "            inputs\n",
    "        )  # Apply layer norm to inputs with epsilon = 1e-5\n",
    "        attn_output = self.attention(\n",
    "            normalized_inputs\n",
    "        )  # Apply the multi-head attention layer\n",
    "        attn_output = self.attn_dropout(\n",
    "            attn_output, training=training\n",
    "        )  # Apply Dropout with rate attn_dropout_rate:\n",
    "        x = inputs + attn_output  # Add a residual connection:\n",
    "\n",
    "        # Process through the Feed-Forward Network:\n",
    "        ffn_output = self.ffn(x, training=training)  # Compute the feed-forward output\n",
    "        output = x + ffn_output  # Apply a second residual connection\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim, attention_dim, feedforward_dim, num_heads, num_blocks\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A stack of Transformer encoder blocks.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "            num_blocks: Number of encoder blocks.\n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # BEGIN SOLUTION\n",
    "        self.encoder_blocks = [\n",
    "            TransformerEncoderBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                attention_dim=attention_dim,\n",
    "                feedforward_dim=feedforward_dim,\n",
    "                num_heads=num_heads,\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ]\n",
    "        # END SOLUTION\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer encoder.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "            output: The final output of the encoder (batch_size, seq_length, embed_dim).\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, image_size, patch_size, input_channels, embed_dim):\n",
    "        \"\"\"\n",
    "        Converts an image into a sequence of patch embeddings.\n",
    "\n",
    "        Args:\n",
    "            image_size: Size (height/width) of the input image (assumed square).\n",
    "            patch_size: Size of each (square) patch.\n",
    "            input_channels: Number of channels in the input image.\n",
    "            embed_dim: Dimensionality of the patch embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.input_channels = input_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        assert (\n",
    "            image_size % patch_size == 0\n",
    "        ), f\"Image size {image_size} must be divisible by patch size {patch_size}\"\n",
    "\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.projection = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def call(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass for patch embedding.\n",
    "\n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, height, width, input_channels).\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, num_patches, embed_dim).\n",
    "        \"\"\"\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "\n",
    "        patches = tf.reshape(\n",
    "            patches,\n",
    "            [\n",
    "                tf.shape(patches)[0],\n",
    "                -1,\n",
    "                self.patch_size * self.patch_size * self.input_channels,\n",
    "            ],\n",
    "        )\n",
    "        patch_embeddings = self.projection(patches)\n",
    "\n",
    "        return patch_embeddings\n",
    "\n",
    "\n",
    "# Decorator is to be able to save the model\n",
    "@register_keras_serializable()\n",
    "class VisionTransformer(tf.keras.Model):\n",
    "    # We'll give you the init ;)\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        patch_size,\n",
    "        num_heads,\n",
    "        num_blocks,\n",
    "        embed_dim,\n",
    "        attention_dim,\n",
    "        feedforward_dim,\n",
    "        input_size=(28, 28, 1),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Vision Transformer model implementation.\n",
    "\n",
    "        Args:\n",
    "            num_classes: Number of output classes.\n",
    "            patch_size: Size of each patch.\n",
    "            num_heads: Number of attention heads.\n",
    "            num_blocks: Number of Transformer encoder blocks.\n",
    "            embed_dim: Dimensionality of the patch/position embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            input_size: Dimensionality of the input\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        image_size = input_size[0]\n",
    "        input_channels = input_size[-1]\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_size, patch_size, input_channels, embed_dim\n",
    "        )\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Positional embedding for each patch\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # Learnable class token used for classification\n",
    "        self.cls = self.add_weight(\n",
    "            \"cls\", shape=(1, 1, embed_dim), initializer=tf.random_normal_initializer()\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            embed_dim=embed_dim,\n",
    "            attention_dim=attention_dim,\n",
    "            feedforward_dim=feedforward_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "        )\n",
    "\n",
    "        # Classification head: LayerNorm -> Dense\n",
    "        self.classification_head = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.LayerNormalization(epsilon=1e-5),\n",
    "                tf.keras.layers.Dense(num_classes),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass through the Vision Transformer model.\n",
    "\n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, height, width, input_channels).\n",
    "\n",
    "        Returns:\n",
    "            logits: Logits of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        # Compute Patch Embeddings: Pass images through self.patch_embedding to\n",
    "        # obtain patch_embeddings of shape (batch_size, num_patches, embed_dim)\n",
    "        patch_embeddings = self.patch_embedding(images)\n",
    "\n",
    "        # Add Positional Embeddings:\n",
    "        # Generate a sequence of positions (of length num_patches) and map them\n",
    "        # through self.position_embedding to obtain positional embeddings:\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        positional_embeddings = self.position_embedding(positions)\n",
    "        # positional_embeddings of shape (1, num_patches, embed_dim)\n",
    "        positional_embeddings = tf.expand_dims(positional_embeddings, axis=0)\n",
    "        # Add these positional embeddings to patch_embeddings elementwise.\n",
    "        transformer_input = patch_embeddings + positional_embeddings\n",
    "\n",
    "        # Prepend the Class Token:\n",
    "        # Broadcast the learnable class token to match the batch size,\n",
    "        # resulting in a tensor of shape (batch_size, 1, embed_dim).\n",
    "        cls_token = tf.broadcast_to(self.cls, [batch_size, 1, self.embed_dim])\n",
    "        # Concatenate this class token with the positional-enhanced\n",
    "        # patch embeddings along the sequence dimension, yielding:\n",
    "        transformer_input = tf.concat([cls_token, transformer_input], axis=1)\n",
    "\n",
    "        # Apply the Transformer Encoder:\n",
    "        # Pass transformer_input through self.transformer_encoder which outputs encoder_output\n",
    "        # with shape (batch_size, num_patches + 1, embed_dim)\n",
    "        encoded = self.transformer_encoder(transformer_input)\n",
    "\n",
    "        # Classification:\n",
    "        # The classification head uses the embedding corresponding to the class token (index 0)\n",
    "        # to compute logits, with shape(batch_size, num_classes)\n",
    "        logits = self.classification_head(encoded[:, 0, :])\n",
    "\n",
    "        # The model thus returns the logits that represent the class scores for each input image.\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
