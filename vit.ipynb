{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import register_keras_serializable\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SingleHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        \"\"\"\n",
    "        Single-head attention layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimensionality for queries, keys, and values.\n",
    "        \"\"\"\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        # hidden_dim: The dimension of the projected tensors.\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # W_value: A dense layer mapping inputs to tensors with dimension hidden_dim\n",
    "        self.W_query = tf.keras.layers.Dense(hidden_dim)\n",
    "        # W_key: A dense layer mapping inputs to tensors with dimension hidden_dim\n",
    "        self.W_key = tf.keras.layers.Dense(hidden_dim)\n",
    "        # W_value: A dense layer mapping inputs to tensors with dimension hidden_dim\n",
    "        self.W_value = tf.keras.layers.Dense(hidden_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for single-head attention.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            attention_output: Attention output of shape (batch_size, seq_length, hidden_dim).\n",
    "        \"\"\"\n",
    "        # Linear Projections\n",
    "        Q = self.W_query(inputs)\n",
    "        K = self.W_key(inputs)\n",
    "        V = self.W_value(inputs)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attention_scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(\n",
    "            tf.cast(self.hidden_dim, tf.float32)\n",
    "        )\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        attention_output = tf.matmul(attention_weights, V)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, head_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Multi-head attention layer.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimensionality of the input and final output.\n",
    "            head_dim: Dimensionality for each attention head.\n",
    "            num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        # Each head is an instance of your single-head attention layer,\n",
    "        # configured with a projection output dimensionality of head_dim.\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Create a list of attention heads. Each head is an instance of your single-head attention layer,\n",
    "        # configured with a projection output dimensionality of head_dim.\n",
    "        self.heads = [SingleHeadAttention(head_dim) for _ in range(num_heads)]\n",
    "\n",
    "        # Define a Dense layer that projects the concatenated outputs\n",
    "        # back to the original input dimension (input_dim).\n",
    "        self.output_projection = tf.keras.layers.Dense(input_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            multihead_output: Multi-head attention output of shape (batch_size, seq_length, input_dim).\n",
    "        \"\"\"\n",
    "        head_outputs = [head(inputs) for head in self.heads]\n",
    "        concat_output = tf.concat(head_outputs, axis=-1)\n",
    "        multihead_output = self.output_projection(concat_output)\n",
    "\n",
    "        return multihead_output\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        attention_dim,\n",
    "        feedforward_dim,\n",
    "        num_heads,\n",
    "        ffn_dropout_rate=0.1,\n",
    "        attn_dropout_rate=0.1,\n",
    "        epsilon=1e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A single Transformer encoder block with multi-head attention and feed-forward network.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        # for call feature later\n",
    "        self.attn_norm = tf.keras.layers.LayerNormalization(epsilon=epsilon)\n",
    "\n",
    "        # Create a MultiHeadAttentionLayer that processes the inputs\n",
    "        # to produce attn_output = MultiHeadAttentionLayer(inputs)\n",
    "        self.attention = MultiHeadAttentionLayer(\n",
    "            input_dim=embed_dim, head_dim=attention_dim, num_heads=num_heads\n",
    "        )\n",
    "        # for call feature later\n",
    "        self.attn_dropout = tf.keras.layers.Dropout(attn_dropout_rate)\n",
    "\n",
    "        # Build a Sequential block consisting of:\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.LayerNormalization(\n",
    "                    epsilon=epsilon\n",
    "                ),  # LayerNormalization with epsilon = 1e-5\n",
    "                tf.keras.layers.Dense(\n",
    "                    feedforward_dim, activation=tf.nn.gelu\n",
    "                ),  # A Dense layer with GELU activation mapping to feedforward_dim units\n",
    "                tf.keras.layers.Dropout(\n",
    "                    ffn_dropout_rate\n",
    "                ),  # A Dropout layer with ffn_dropout_rate.\n",
    "                tf.keras.layers.Dense(\n",
    "                    embed_dim\n",
    "                ),  # A Dense layer mapping back to embed_dim\n",
    "                tf.keras.layers.Dropout(\n",
    "                    ffn_dropout_rate\n",
    "                ),  # A Dropout layer with ffn_dropout_rate.\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass for a single Transformer encoder block.\n",
    "        \"\"\"\n",
    "        # Compute the Multi-Head Attention\n",
    "        normalized_inputs = self.attn_norm(\n",
    "            inputs\n",
    "        )  # Apply layer norm to inputs with epsilon = 1e-5\n",
    "        attn_output = self.attention(\n",
    "            normalized_inputs\n",
    "        )  # Apply the multi-head attention layer\n",
    "        attn_output = self.attn_dropout(\n",
    "            attn_output, training=training\n",
    "        )  # Apply Dropout with rate attn_dropout_rate:\n",
    "        x = inputs + attn_output  # Add a residual connection:\n",
    "\n",
    "        # Process through the Feed-Forward Network:\n",
    "        ffn_output = self.ffn(x, training=training)  # Compute the feed-forward output\n",
    "        output = x + ffn_output  # Apply a second residual connection\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim, attention_dim, feedforward_dim, num_heads, num_blocks\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A stack of Transformer encoder blocks.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            num_heads: Number of attention heads.\n",
    "            num_blocks: Number of encoder blocks.\n",
    "        \"\"\"\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        # BEGIN SOLUTION\n",
    "        self.encoder_blocks = [\n",
    "            TransformerEncoderBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                attention_dim=attention_dim,\n",
    "                feedforward_dim=feedforward_dim,\n",
    "                num_heads=num_heads,\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ]\n",
    "        # END SOLUTION\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer encoder.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, seq_length, embed_dim).\n",
    "\n",
    "        Returns:\n",
    "            output: The final output of the encoder (batch_size, seq_length, embed_dim).\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, image_size, patch_size, input_channels, embed_dim):\n",
    "        \"\"\"\n",
    "        Converts an image into a sequence of patch embeddings.\n",
    "\n",
    "        Args:\n",
    "            image_size: Size (height/width) of the input image (assumed square).\n",
    "            patch_size: Size of each (square) patch.\n",
    "            input_channels: Number of channels in the input image.\n",
    "            embed_dim: Dimensionality of the patch embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.input_channels = input_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        assert (\n",
    "            image_size % patch_size == 0\n",
    "        ), f\"Image size {image_size} must be divisible by patch size {patch_size}\"\n",
    "\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.projection = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def call(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass for patch embedding.\n",
    "\n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, height, width, input_channels).\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, num_patches, embed_dim).\n",
    "        \"\"\"\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "\n",
    "        patches = tf.reshape(\n",
    "            patches,\n",
    "            [\n",
    "                tf.shape(patches)[0],\n",
    "                -1,\n",
    "                self.patch_size * self.patch_size * self.input_channels,\n",
    "            ],\n",
    "        )\n",
    "        patch_embeddings = self.projection(patches)\n",
    "\n",
    "        return patch_embeddings\n",
    "\n",
    "\n",
    "# Decorator is to be able to save the model\n",
    "@register_keras_serializable()\n",
    "class VisionTransformer(tf.keras.Model):\n",
    "    # We'll give you the init ;)\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        patch_size,\n",
    "        num_heads,\n",
    "        num_blocks,\n",
    "        embed_dim,\n",
    "        attention_dim,\n",
    "        feedforward_dim,\n",
    "        input_size=(28, 28, 1),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Vision Transformer model implementation.\n",
    "\n",
    "        Args:\n",
    "            num_classes: Number of output classes.\n",
    "            patch_size: Size of each patch.\n",
    "            num_heads: Number of attention heads.\n",
    "            num_blocks: Number of Transformer encoder blocks.\n",
    "            embed_dim: Dimensionality of the patch/position embeddings.\n",
    "            attention_dim: Dimensionality used in each attention head.\n",
    "            feedforward_dim: Dimensionality of the intermediate feed-forward layer.\n",
    "            input_size: Dimensionality of the input\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        image_size = input_size[0]\n",
    "        input_channels = input_size[-1]\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_size, patch_size, input_channels, embed_dim\n",
    "        )\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Positional embedding for each patch\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # Learnable class token used for classification\n",
    "        self.cls = self.add_weight(\n",
    "            \"cls\", shape=(1, 1, embed_dim), initializer=tf.random_normal_initializer()\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            embed_dim=embed_dim,\n",
    "            attention_dim=attention_dim,\n",
    "            feedforward_dim=feedforward_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "        )\n",
    "\n",
    "        # Classification head: LayerNorm -> Dense\n",
    "        self.classification_head = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.LayerNormalization(epsilon=1e-5),\n",
    "                tf.keras.layers.Dense(num_classes),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass through the Vision Transformer model.\n",
    "\n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, height, width, input_channels).\n",
    "\n",
    "        Returns:\n",
    "            logits: Logits of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        # Compute Patch Embeddings: Pass images through self.patch_embedding to\n",
    "        # obtain patch_embeddings of shape (batch_size, num_patches, embed_dim)\n",
    "        patch_embeddings = self.patch_embedding(images)\n",
    "\n",
    "        # Add Positional Embeddings:\n",
    "        # Generate a sequence of positions (of length num_patches) and map them\n",
    "        # through self.position_embedding to obtain positional embeddings:\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        positional_embeddings = self.position_embedding(positions)\n",
    "        # positional_embeddings of shape (1, num_patches, embed_dim)\n",
    "        positional_embeddings = tf.expand_dims(positional_embeddings, axis=0)\n",
    "        # Add these positional embeddings to patch_embeddings elementwise.\n",
    "        transformer_input = patch_embeddings + positional_embeddings\n",
    "\n",
    "        # Prepend the Class Token:\n",
    "        # Broadcast the learnable class token to match the batch size,\n",
    "        # resulting in a tensor of shape (batch_size, 1, embed_dim).\n",
    "        cls_token = tf.broadcast_to(self.cls, [batch_size, 1, self.embed_dim])\n",
    "        # Concatenate this class token with the positional-enhanced\n",
    "        # patch embeddings along the sequence dimension, yielding:\n",
    "        transformer_input = tf.concat([cls_token, transformer_input], axis=1)\n",
    "\n",
    "        # Apply the Transformer Encoder:\n",
    "        # Pass transformer_input through self.transformer_encoder which outputs encoder_output\n",
    "        # with shape (batch_size, num_patches + 1, embed_dim)\n",
    "        encoded = self.transformer_encoder(transformer_input)\n",
    "\n",
    "        # Classification:\n",
    "        # The classification head uses the embedding corresponding to the class token (index 0)\n",
    "        # to compute logits, with shape(batch_size, num_classes)\n",
    "        logits = self.classification_head(encoded[:, 0, :])\n",
    "\n",
    "        # The model thus returns the logits that represent the class scores for each input image.\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
