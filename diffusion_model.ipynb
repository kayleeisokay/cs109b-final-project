{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d2a191",
   "metadata": {},
   "source": [
    "## Diffusion Model using U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e333f9",
   "metadata": {},
   "source": [
    "This is an appendix notebook that is optional to the reader. It contains code for training a diffusion model using a U-Net architecture. The idea was to train on the `disgust` category to upsample for our models but it ultimately failed. However, we learned a lot about diffusion models and Kaylee finally found her spark after a decade of searching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd835f07",
   "metadata": {},
   "source": [
    "*Citing our sources, parts of this notebook was AI assisted via DeepSeek. The functions marked with \"AI\" were AI assisted.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe18bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinisitc, relative embeddings\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        # had to use device because it was giving errors\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f7237",
   "metadata": {},
   "source": [
    "The positional embeddings, just like in the transformer paper (Vaswani et al., 2017) are defined as follows:\n",
    "\n",
    "$$\n",
    "PE_{pos}(t) = \\begin{cases}\n",
    "\\sin(\\frac{t}{10000^{\\frac{2i}{d}}}) & \\text{if } i \\text{ is even} \\\\\n",
    "\n",
    "\\cos(\\frac{t}{10000^{\\frac{2i-1}{d}}}) & \\text{if } i \\text{ is odd}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The pytorch code looks slightly different but it is equivalent. We use a similar approach to (Kubo, 2024) as a more efficient way to compute the positional embeddings. These embeddings are non-trainable and represent relative positions in a time sequence. This is in contrast to vision transformers, which have learnable positional embeddings. \n",
    "\n",
    "Positional embeddings are used to encode the time step information into the model. These embeddings are fed into the residual blocks of the U-Net as a additional feature map. This is akin to conditional generative adversarial networks (cGANs) where the condition is added as an additional feature map.\n",
    "\n",
    "Next, we create a class for the attention block The attention block is the standard attention block we have all seen before. The only difference is that we use a linear layer instead of a convolutional layer to compute the query, key and value matrices. The attention block allows the model to focus on different parts of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999735cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very similar to HW5 attention block\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch, channels, height, width\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Q, K, V projections\n",
    "        query = self.query(x).view(B, -1, H * W)\n",
    "        key = self.key(x).view(B, -1, H * W)\n",
    "        value = self.value(x).view(B, -1, H * W)\n",
    "\n",
    "        attention_map = torch.matmul(query.transpose(1, 2), key)\n",
    "        attention_map = self.softmax(attention_map)\n",
    "        out = torch.matmul(attention_map, value.transpose(1, 2))\n",
    "\n",
    "        out = out.transpose(1, 2).view(B, C, H, W)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2833f",
   "metadata": {},
   "source": [
    "For the next class, we create a residual block. A lot of the code was taken from a medium post (Dz, 2023). The architecture as defined in the init contains two convolutional layers, a group normalization, attention block and a skip connection. Group norm is essentially identical to batch norm but it is batch size agnostic. For all intents and purposes, one can view it as a bathch norm layer. \n",
    "\n",
    "In the forward pass, we apply convolution, project the time embedding and add it as an additional feature map. We then apply convolution again and pass the result through an attention block. The final step is a skip connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d20482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block with Time Embedding and Attention\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.act1 = nn.SiLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "        self.act2 = nn.SiLU()\n",
    "\n",
    "        # Inject the time embedding into the block\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
    "        self.attn = AttentionBlock(out_channels)\n",
    "        self.res_conv = (\n",
    "            nn.Conv2d(in_channels, out_channels, 1)\n",
    "            if in_channels != out_channels\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.act1(self.norm1(self.conv1(x)))\n",
    "        # Add the time embedding as another feature map\n",
    "        h = h + self.time_mlp(t_emb).view(t_emb.size(0), -1, 1, 1)\n",
    "        h = self.act2(self.norm2(self.conv2(h)))\n",
    "        h = self.attn(h)\n",
    "        # Add the residual connection\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af6de9",
   "metadata": {},
   "source": [
    "The class below is just a U-Net. There's not much to add here since we used U-Net extensively in the main project. It's the standard encoder, bottleneck, decoder architecture with long skip connections. We alo use residual blocks with attention instead of vanilla convolution blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net Architecture\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        time_emb_dim = 256\n",
    "        self.time_embedding = SinusoidalTimeEmbedding(time_emb_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "        ## Define the architecture\n",
    "        # Encoder\n",
    "        self.enc1 = ResBlock(1, 64, time_emb_dim)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = ResBlock(64, 128, time_emb_dim)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResBlock(128, 256, time_emb_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec1 = ResBlock(256, 128, time_emb_dim)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = ResBlock(128, 64, time_emb_dim)\n",
    "\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    ## Define the forward pass\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embedding(t)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x, t_emb)\n",
    "        p1 = self.pool1(e1)\n",
    "\n",
    "        e2 = self.enc2(p1, t_emb)\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p2, t_emb)\n",
    "\n",
    "        # Decoder\n",
    "        up1 = self.up1(b)\n",
    "        d1 = self.dec1(torch.cat([up1, e2], dim=1), t_emb)\n",
    "\n",
    "        up2 = self.up2(d1)\n",
    "        d2 = self.dec2(torch.cat([up2, e1], dim=1), t_emb)\n",
    "\n",
    "        return self.out(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9effebd",
   "metadata": {},
   "source": [
    "The model has been defined. Now we move on the interesting part, the training loop. First we define the forward process. The forward process is a Markov chain that progressively adds noise to the data. The noise is drawn from a multivariate standard normal. We define the forward process as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "q(x_t | x_{t-1}) = \\mathcal{N}(\\mu_t = \\sqrt{1 - \\beta_t} x_{t-1}, \\sigma_t^2 = \\beta_t I).\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The variance schedule is defined as $\\beta_t$. In the class below, we use a linear schedule. Cosine schedules have been shown to be better but we stick to linear for simplicity (Nichol & Dhariwal, 2021). The variable $x_t$ is the image at time $t$ and by the Markov assumption, the image at time $t$ only depends on the image at time $t-1$. Like shown in lecture, we reparameterize the forward process using the following equations:\n",
    "\n",
    "$$\n",
    "\\alpha_t = 1 - \\beta_t,\n",
    "$$\n",
    "$$\n",
    "\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s.\n",
    "$$\n",
    "\n",
    "Equation 1 now becomes\n",
    "\n",
    "$$\n",
    "q(x_t | x_0) = \\mathcal{N}(\\mu_t = \\sqrt{\\bar{\\alpha}_t} x_0, \\sigma_t^2 = (1 - \\bar{\\alpha}_t) I).\n",
    "$$\n",
    "\n",
    "All variables have been defined. The class below implements the forward process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81155194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward process of the diffusion model\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        # need to be on same device\n",
    "        # was getting same bug as before\n",
    "        device = x_start.device\n",
    "        alphas_cumprod = self.alphas_cumprod.to(device)\n",
    "        sqrt_alpha_cumprod = torch.sqrt(alphas_cumprod[t])[:, None, None, None]\n",
    "        sqrt_one_minus = torch.sqrt(1 - alphas_cumprod[t])[:, None, None, None]\n",
    "\n",
    "        return sqrt_alpha_cumprod * x_start + sqrt_one_minus * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4291a4b3",
   "metadata": {},
   "source": [
    "\n",
    "**Author's Notes**\n",
    "\n",
    "Since the training loop was only introduced in the last lecture, we had to use DeepSeek for help. The general concept was clear but the implementation was not. The tricky part was step 4 and 5 because it was vague on what to do about 'take gradient step on $\\nabla_\\theta || \\epsilon - \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t)||^2$'. The AI was able to clarify this and the code below is the result of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654f130",
   "metadata": {},
   "source": [
    "We follow the steps outlined in the paper \"Denoising Diffusion Probabilistic Models\" (Ho et al., 2020). The steps can be found on slide 42 of the final lecture. The steps are as follows:\n",
    "\n",
    "<img src=\"./img/diffusion_train.png\" width=\"650\" height=\"325\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-assisted\n",
    "class DiffusionModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Denoising model, model to predict the noise\n",
    "        self.model = SimpleUNet()\n",
    "\n",
    "        # Foward process\n",
    "        self.diffusion = Diffusion()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Inputs are x and time step t\n",
    "        return self.model(x, t)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Step 1: sample images\n",
    "        x, _ = batch\n",
    "\n",
    "        # Step 2: Sample time step from a uniform distribution\n",
    "        t = torch.randint(\n",
    "            0, self.diffusion.timesteps, (x.size(0),), device=self.device\n",
    "        ).long()\n",
    "\n",
    "        # Step 3: Sample noise\n",
    "        noise = torch.randn_like(x, device=self.device)\n",
    "\n",
    "        # Step 4: Generates noisy images\n",
    "        x_noisy = self.diffusion.q_sample(x, t, noise)\n",
    "\n",
    "        # Step 5: Predict the noise\n",
    "        noise_pred = self(x_noisy, t)\n",
    "\n",
    "        # Compute the MSE\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prepare the DataLoader for FER-2013 Dataset\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(),  # in case it's not grayscale\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),  # Normalize to [-1, 1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71812282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-05-07 20:44:28.425586: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-07 20:44:28.425632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-07 20:44:28.426940: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-07 20:44:28.434009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-07 20:44:29.224383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | SimpleUNet | 2.4 M  | train\n",
      "---------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.507     Total estimated model params size (MB)\n",
      "81        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f12f76894b54470aad6a01a9ea10cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    }
   ],
   "source": [
    "# Update the path to the FER-2013 dataset on your system\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    \"./data/clean_train_diffusion\", transform=transform\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = DiffusionModel()\n",
    "trainer = pl.Trainer(max_epochs=1000, accelerator=\"auto\")\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc579b1",
   "metadata": {},
   "source": [
    "**Author's Notes**\n",
    "\n",
    "Sampling was trickiest because we barely covered it in the lecture. We knew the general idea is to feed in a noise vector and then iteratively denoise it but how to do so was not clear. The breakthrough occurred when we realized the process can be viewed as solving a stochastic differential equation (SDE), specifically a variance preserving SDE. Framing it this way made it clear why we have a $\\sigma_t$ term and what the denoising equation is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad12ba",
   "metadata": {},
   "source": [
    "Sampling is where we take a noise vector and iteratively denoise it. We do so by using a special equation\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)) + \\sigma_t z.\n",
    "$$\n",
    "\n",
    "The equation takes as input the noise vector $x_t$, time step $t$ and the model prediction $\\epsilon_\\theta(x_t, t)$. The $\\sigma_t$ term is added to introduce noise because the forward process is stochastic and therefore the reverse process must also be stochastic. It ensures mathematical consistency for the stochastic differential equation (SDE) we are solving.\n",
    "\n",
    "\n",
    "<img src=\"./img/diffusion_sample.png\" width=\"650\" height=\"325\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b66696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-assisted\n",
    "@torch.no_grad()\n",
    "def generate_image(\n",
    "    model,\n",
    "    diffusion,\n",
    "    image_size=(1, 48, 48),\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Step 1: Sample random noise\n",
    "    img = torch.randn(1, *image_size, device=device)\n",
    "\n",
    "    for t in reversed(range(diffusion.timesteps)):\n",
    "        t_tensor = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # Predict the noise\n",
    "        noise_pred = model(img, t_tensor)\n",
    "\n",
    "        # Get alpha parameters\n",
    "        alpha_t = diffusion.alphas[t].to(device)\n",
    "        alpha_cumprod_t = diffusion.alphas_cumprod[t].to(device)\n",
    "        beta_t = diffusion.betas[t].to(device)\n",
    "\n",
    "        # Step 4: Apply the noise reversing formula\n",
    "        coef1 = 1 / torch.sqrt(alpha_t)\n",
    "        coef2 = beta_t / torch.sqrt(1 - alpha_cumprod_t)\n",
    "        img = coef1 * (img - coef2 * noise_pred)\n",
    "\n",
    "        # Step 3\n",
    "        # Add noise except for last step\n",
    "        # we want final image to be clean\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(img)\n",
    "            sigma_t = torch.sqrt(beta_t)\n",
    "            img += sigma_t * noise\n",
    "\n",
    "    img = torch.clamp(img, -1.0, 1.0)\n",
    "    img = (img + 1) / 2\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad2d6d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate an image\n",
    "generated_image = generate_image(model, model.diffusion)\n",
    "\n",
    "# Convert to numpy\n",
    "img_np = generated_image.squeeze().cpu().numpy()\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img_np, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e57c8f",
   "metadata": {},
   "source": [
    "The results are subpar. Generating human faces with specific emotions is a very difficult task that most likely cannot be solved with this approach. The image above looks like noise but we can discern a few characteristics of a face. We can see the model is learning the shape of the face and the background. We can see evidence of eyes and a nose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a28640",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a2719",
   "metadata": {},
   "source": [
    "Dz. (2023, September 17). Intro to Diffusion Model â€” Part 5 - DZ - Medium. Medium. https://dzdata.medium.com/intro-to-diffusion-model-part-5-d0af8331871\n",
    "\n",
    "Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. arXiv (Cornell University). https://doi.org/10.48550/arxiv.2006.11239\n",
    "\n",
    "Kubo, H. (2024, December 28). Understanding Transformer sinusoidal Position embedding. Medium. https://medium.com/%40hirok4/understanding-transformer-sinusoidal-position-embedding-7cbaaf3b9f6a\n",
    "\n",
    "Nichol, A., & Dhariwal, P. (2021, February 18). Improved denoising diffusion probabilistic models. arXiv.org. https://arxiv.org/abs/2102.09672\n",
    "\n",
    "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017, June 12). Attention is all you need. arXiv.org. https://arxiv.org/abs/1706.03762"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
